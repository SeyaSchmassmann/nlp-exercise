{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 01\n",
    "\n",
    "Content:\n",
    "1. Softmax\n",
    "2. Byte-Pair\n",
    "3. Tokenizer\n",
    "4. NLP-Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Rem: you get bonus for this exercise if you answer at least 3 out of 4 questions.\n",
    "\n",
    "Here we are going to answer the question, why softmax is called \"softmax\" and investigate softmax characteristics. The $i \\text{-th}$ component of the softmax is given by:\n",
    "\n",
    "$$f(x_i) := \\text{softmax}(x_i) = \\frac{\\exp x_i}{ \\sum_{j = 1}^k \\exp x_j}$$\n",
    "\n",
    "\n",
    "And thus the vector is $f(\\mathbf x) = [f(x_1), \\dots, f(x_k)]$ where $\\mathbf x = (x_1, \\dots, x_k)$.\n",
    "\n",
    "\n",
    "1. show that $f(\\mathbf x)$ can be interpreted as a probability. To do so, show that $\\sum_{i=1}^k f(x_i) = 1$ and $f(x) > 0 \\quad \\forall x \\in \\mathbb{R}$\n",
    "2. show that softmax is $C^\\infty$, i.e. that you can calculate the derivative of $f(x_i)$ as often as you want with respect to $x_i$. You can use the fact that $\\exp x$ is smooth (smooth means $C ^ \\infty$)\n",
    "3. show that if $x_i \\lt x_j$ then $f(x_i) \\lt f(x_j)$. Does the converse hold as well?\n",
    "4. what is the limit of $f(x_i)$ for $x_i \\to + \\infty$? Same question for $x \\to - \\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex01](week_01_ex01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding\n",
    "\n",
    "Implement Byte-paid encoding and reproduce the example in chapter 2.5.2 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Byte Pair Encoding\"](byte_pair_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'd',\n",
       " 'd ',\n",
       " 'e',\n",
       " 'ed ',\n",
       " 'ed b',\n",
       " 'ed br',\n",
       " 'ed bre',\n",
       " 'ed f',\n",
       " 'f',\n",
       " 'n',\n",
       " 'r',\n",
       " 't'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bytePairEncoding(corpus, k):\n",
    "  vocab = set(corpus)\n",
    "  corpus = list(corpus)\n",
    "  \n",
    "  for _ in range(k):\n",
    "    pairs = {}\n",
    "    for tokenLeft, tokenRight in zip(corpus[:-1], corpus[1:]):\n",
    "      if (tokenLeft, tokenRight) in pairs:\n",
    "        pairs[(tokenLeft, tokenRight)] += 1\n",
    "      else:\n",
    "        pairs[(tokenLeft, tokenRight)] = 1\n",
    "\n",
    "    (tokenLeft, tokenRight) = max(pairs, key=pairs.get)\n",
    "\n",
    "    vocab.add(tokenLeft + tokenRight)\n",
    "\n",
    "    i = 0\n",
    "    toRemoveIndeces = []\n",
    "    while i < len(corpus) - 1:\n",
    "      if corpus[i] == tokenLeft and corpus[i + 1] == tokenRight:\n",
    "        corpus[i] = tokenLeft + tokenRight\n",
    "        toRemoveIndeces.append(i + 1)\n",
    "        i += 2\n",
    "      else:\n",
    "        i += 1\n",
    "\n",
    "    toRemoveIndeces.reverse()\n",
    "    for i in toRemoveIndeces:\n",
    "      del corpus[i]\n",
    "\n",
    "  return vocab\n",
    "\n",
    "corpus = \"fred fed ted bread and ted fed fred bread\"\n",
    "bytePairEncoding(corpus, 6)\n",
    "\n",
    "\n",
    "corpus = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Go through the HuggingFace tutorial on [tokenizers](https://huggingface.co/learn/nlp-course/chapter2/4) and answer:\n",
    "\n",
    "- **why do we need tokenizers?** <br>\n",
    "  Tokenizers are splitting a text into tokens and convert them into a format that can be unserstood and processed by machines.\n",
    "- **what is the difference between a character-based, word-based and a subword-based tokenizer? What are the advantages and disadvantages of each?** <br>\n",
    "  Tokenizers can split a text into tokens by various cirterias. They could treat every character as a token, split a text into the its words, or split a text into subwords. While the first two approaches are self-explanatory, the subword-based tokenizer is a bit more advanced. Frequently used words are not split into subwords, but rare words are decomposed into meaningful more frequent subwords.\n",
    "  Character-based tokenizers lead to a smaller vocabulary, because with single letters every word can be composed. But a single vocabulary item does not have much semantic meaning, since it is a single letter. Word-based tokenizers will use the words as vocabulary, which means the vocabulary has a lot meaning (words have a meaning) but the vocabulary size is very large. Subword tokenizers are a compromise between the two. They typically have a smaller vocabulary size than word-based tokenizers. Ideally unknown words and typos can be handled by subword tokenizers, since they can be decomposed into subwords that are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "\n",
    "Recall the NLP-Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%| echo: false\n",
    "flowchart TD\n",
    "    A[Data Acquistion] --> B[Preprocessing and Normalization]\n",
    "    B --> C[Modelling]\n",
    "    C --> C\n",
    "    C --> D[Model evaluation]\n",
    "    D --> |more preprocessing needed| B\n",
    "    D --> |more/different data needed| A\n",
    "    D --> E[Added Value]\n",
    "    E --> |reiterate| A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to do some prepocessing and normalization steps. Your task is to do:\n",
    "\n",
    "\n",
    "\n",
    "1. **Data Collection**: Collect a corpus of text data. It is completely up to you.\n",
    "2. **Data Cleaning**: Clean the collected data by removing any irrelevant information such as HTML tags, URLs, numbers, etc. This step depends in the corpus you chose:\n",
    "    - count the vocabulary size before and after cleaning\n",
    "3. **Tokenization**: Apply a tokenizer (e.g. using https://www.nltk.org/):\n",
    "    - count the vocabulary size before and after tokenization\n",
    "    - how much time is needed per word in average?\n",
    "4. **Stopwords Removal**: Identify and remove stopwords from the tokens. Stopwords are common words that do not contribute much to the meaning of a sentence, such as 'the', 'is', 'in', etc.\n",
    "    - count the vocabulary size before and after stopword removal\n",
    "5. **Stemming and Lemmatization**: Apply stemming and lemmatization techniques to the tokens and observe the differences. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. Lemmatization, on the other hand, reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis.\n",
    "    - count the vocabulary size for the stemmed and lemmatized vocabulary\n",
    "6. **Compare**: \n",
    "    - create a barplot of the results you have collected above (https://plotly.com/python/bar-charts/)\n",
    "\n",
    "Useful libraries:\n",
    "\n",
    "- NLTK, Spacy\n",
    "- plotly or matplotlib for plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
