{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02\n",
    "\n",
    "Content:\n",
    "\n",
    "1. Language modelling using bigrams\n",
    "2. Necessity of end of sentence token `<\\s>`\n",
    "3. Doing k-smoothing, we are still dealing with a probability\n",
    "4. Makemore Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language modelling using bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Seya.Schmassmann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Seya.Schmassmann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "emma = nltk.corpus.gutenberg.raw(fileids='austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "print(emma[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.',\n",
       " \"she was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\",\n",
       " 'her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.',\n",
       " \"sixteen years had miss taylor been in mr. woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of emma.\",\n",
       " 'between _them_ it was more the intimacy\\nof sisters.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, sent_tokenize\n",
    "sentences = sent_tokenize(emma.lower())\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- count all bigrams in sentences above (add a start and end of sentence token), since we want to generate new sentences\n",
    "- construct probabilities from the bigram counter above\n",
    "- to generate new sentences you start with `<s>` and generate a new word from the bigram probabilities until you reach the end of sentence token `<\\s>`\n",
    "- finally calculate the perplexity of your generated sentences and do the same for original sentences from `sentences`\n",
    "\n",
    "\n",
    "Tipps:\n",
    "- `nltk.utils` has a `ngrams` function\n",
    "- you might want to use `from collections import Counter`\n",
    "- `np.random.choice(candidate_tokens, size=1, p=candidate_probs)` can be used to draw tokens given computed probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', '[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', '</s>'], ['<s>', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', '</s>'], ['<s>', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.', '</s>'], ['<s>', 'sixteen', 'years', 'had', 'miss', 'taylor', 'been', 'in', 'mr.', 'woodhouse', \"'s\", 'family', ',', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', ',', 'very', 'fond', 'of', 'both', 'daughters', ',', 'but', 'particularly', 'of', 'emma', '.', '</s>'], ['<s>', 'between', '_them_', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.', '</s>']]\n",
      "[(('.', '</s>'), 5145), ((',', 'and'), 1882), ((\"''\", '</s>'), 1567), (('<s>', '``'), 1378), (('.', \"''\"), 1158)]\n",
      "0.8098536124665512\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# add start and end tokens\n",
    "tokenized_sentences = [[\"<s>\"] + word_tokenize(sentence) + [\"</s>\"] for sentence in sentences]\n",
    "print(tokenized_sentences[:5])\n",
    "\n",
    "# count bigrams\n",
    "bigrams = [tuple(pair) for sentence in tokenized_sentences for pair in ngrams(sentence, 2)]\n",
    "bigram_counts = Counter(bigrams)\n",
    "print(bigram_counts.most_common(5))\n",
    "\n",
    "# compute probabilities\n",
    "bigram_probabilities = {}\n",
    "unigram_counts = Counter([word for sentence in tokenized_sentences for word in sentence])\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    bigram_probabilities[(w1, w2)] = count / unigram_counts.get(w1, 1) # C(w1, w2) / C(w1)\n",
    "print(bigram_probabilities[(\".\", \"</s>\")])\n",
    "\n",
    "\n",
    "def generate_sentence():\n",
    "    sentence = [\"<s>\"]\n",
    "    while sentence[-1] != \"</s>\":\n",
    "        candidates = [w2 for (w1, w2) in bigram_probabilities.keys() if w1 == sentence[-1]]\n",
    "        probs = [bigram_probabilities[(sentence[-1], w2)] for w2 in candidates]\n",
    "        if not candidates:\n",
    "            break\n",
    "        next_word = np.random.choice(candidates, p=probs)\n",
    "        sentence.append(next_word)\n",
    "    return \" \".join(sentence[1:-1])\n",
    "\n",
    "\n",
    "def calculate_perplexity(sentence):\n",
    "    tokens = [\"<s>\"] + word_tokenize(sentence) + [\"</s>\"]\n",
    "    bigrams = list(nltk.ngrams(tokens, 2))\n",
    "    prob = 1\n",
    "    N = len(bigrams)\n",
    "    for w1, w2 in bigrams:\n",
    "        prob *= bigram_probabilities.get((w1, w2), 1e-6)\n",
    "    return prob ** (-1/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generierung von neuen SÃ¤tzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: `` the whole fortnight ; and was wishing any feeling the sort , you will be summer , there is frank churchill !\n",
      "Perplexity of Generated Sentence: 35.62228643090355\n",
      "Perplexity of Original Sentences: 80.28897429408843\n"
     ]
    }
   ],
   "source": [
    "generated_sentence = generate_sentence()\n",
    "print(\"Generated Sentence:\", generated_sentence)\n",
    "print(\"Perplexity of Generated Sentence:\", calculate_perplexity(generated_sentence))\n",
    "print(\"Perplexity of Original Sentences:\", calculate_perplexity(\"Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Necessity of `<\\s>`\n",
    "\n",
    "Exercise 3.5 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/). Given a trainings corpus without `<\\s>`\n",
    "\n",
    "`<s> a b` <br>\n",
    "`<s> b a` <br>\n",
    "`<s> a a` <br>\n",
    "`<s> b b` <br>\n",
    "\n",
    "1. Use a bigram model and calculate the probability of all possible sentences with two words $\\{a, b\\}$. Show that the sum of all these probabilities add up to 1.\n",
    "2. Do the same for all possible sentences of lengths 3 of the words $\\{a, b\\}$. Show that these sum up to 1 as well.\n",
    "\n",
    "The point is, that this property is not very useful: we rather have a language model, that is able to produce a sentence of arbitrary length. When you are generating a sentence, you are not going to decide before hand, how many words you are going to use..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "First let's write down the formula for bigrams:\n",
    "\n",
    "$$\n",
    "\\mathbb P [ <\\! s \\! >, w_1, \\dots, w_n] = \\mathbb P [w_1 \\mid <\\!s \\! > ] \\mathbb P[w_2 \\mid w_1] \\cdots \\mathbb P[w_n \\mid w_{n-1}]\n",
    "$$\n",
    "\n",
    "here $n=2$ for part 1 and $n=3$ for part 2.\n",
    "\n",
    "Let's write $\\mathbb P[b \\mid a] = p_{b|a}$ and use $s$ instead of `<s>`, then from the training corpus we get:\n",
    "\n",
    "\n",
    "- $p_{a|s} = p_{b|s} = \\dots$\n",
    "- $\\dots$\n",
    "\n",
    "Recall that we are counting the number of occurrences, e.g.: $p_{b|a}= \\frac{C(a,b)}{C(a)} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 2\n",
    "\n",
    "We must calculate all probabilities of sentences of lengths 3:\n",
    "\n",
    "- How many are there?\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-smoothing is still a probability\n",
    "\n",
    "Given a vocabulary $V$ and a bigram language model, applying k-smoothing (or additive smoothing), i.e.\n",
    "\n",
    "$$\\mathbb{P}[w_n \\mid w_{n-1}] = \\frac{C(w_n, w_{n-1}) + k}{ C(w_{n-1}) + k|V|}$$\n",
    "\n",
    "we still have a probability.\n",
    "\n",
    "We have to show, that when summing over all possible $w_n$, the sum still adds up to $1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\sum_{w_n \\in V} \\mathbb P [w_n \\mid w_{n-1}] \n",
    "        &=\\sum_{w_n \\in V} \\frac{C(w_{n-1}, w_n) + k}{ C(w_{n-1}) + k|V|} \\\\\n",
    "        &= \\dots = 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Makemore Part I\n",
    "\n",
    "Watch Part I of [makemore](https://youtu.be/PaCmpygFfXo?si=7PonsCOBoNCcmWHo), until 1:02:56, and code along. We will come back to this video series in later weeks.\n",
    "\n",
    "- Write a function, that takes a prefix and completes this prefix according to the the implemented bigram-model (e.g. given the prefix `ann` possible completions could be `anna`, `anner`, `ann...`) \n",
    "- Loglikelihood:\n",
    "    - what happens to the loglikelihood if we use large smoothing factor $k$? \n",
    "    - Calculate the loglikelihood for all words for different $k$ values.\n",
    "    - make an appropriate plot with negative loglikelihood on the y-axis and logarithmic $k$ on the x-axis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
